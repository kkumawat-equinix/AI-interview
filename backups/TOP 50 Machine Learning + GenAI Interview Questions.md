‚öôÔ∏è 1. Core Machine Learning (Concepts)

1Ô∏è‚É£ What is overfitting?
When a model performs well on training data but poorly on unseen data.
‚úÖ Fix: regularization, dropout, early stopping, more data.

2Ô∏è‚É£ What is underfitting?
When a model is too simple to learn underlying patterns.
‚úÖ Fix: use a more complex model or better features.

3Ô∏è‚É£ What is bias‚Äìvariance tradeoff?
Balancing model simplicity (bias) and complexity (variance) to achieve generalization.

4Ô∏è‚É£ What is cross-validation?
Splitting data into multiple folds to test model performance across subsets.

5Ô∏è‚É£ What are hyperparameters?
External settings controlling model training (e.g., learning rate, tree depth).

6Ô∏è‚É£ What is regularization?
Adding a penalty term (L1/L2) to reduce overfitting.

7Ô∏è‚É£ Difference between bagging and boosting?
Bagging trains models in parallel (e.g., Random Forest);
Boosting trains sequentially, focusing on previous errors (e.g., XGBoost).

8Ô∏è‚É£ What‚Äôs the difference between precision and recall?
Precision: correctness of positives.
Recall: coverage of actual positives.

9Ô∏è‚É£ What is ROC-AUC?
Area under ROC curve ‚Äî measures the tradeoff between TPR and FPR.

üîü What is data leakage?
When test information leaks into training ‚Äî causes unrealistic performance.

üìä 2. Algorithms & Models

11Ô∏è‚É£ How does logistic regression work?
Applies a sigmoid function to map linear combination of features to probabilities.

12Ô∏è‚É£ What is a decision tree?
Splits data using feature thresholds to minimize impurity (Gini/Entropy).

13Ô∏è‚É£ What is gradient boosting?
Sequentially builds trees on residual errors from previous ones.

14Ô∏è‚É£ What is KNN?
Classifies points based on nearest k data points in feature space.

15Ô∏è‚É£ What is PCA and why is it used?
Dimensionality reduction technique that finds orthogonal components maximizing variance.

üß† 3. Deep Learning

16Ô∏è‚É£ What is a neural network?
A series of layers that transform input data using learnable weights and nonlinear activations.

17Ô∏è‚É£ What is an activation function?
Adds non-linearity (ReLU, sigmoid, tanh).

18Ô∏è‚É£ What is dropout?
Randomly deactivates neurons during training to prevent overfitting.

19Ô∏è‚É£ What is batch normalization?
Normalizes activations between layers to stabilize training.

20Ô∏è‚É£ What is gradient descent?
An optimization algorithm that updates weights by minimizing loss function.

üß© 4. Natural Language Processing (NLP)

21Ô∏è‚É£ What is tokenization?
Breaking text into words, subwords, or tokens (BPE, WordPiece).

22Ô∏è‚É£ What is word embedding?
Vector representation of words (e.g., Word2Vec, GloVe, BERT embeddings).

23Ô∏è‚É£ What is attention mechanism?
Computes weighted importance of input tokens in context (core of Transformers).

24Ô∏è‚É£ What is BERT?
Bidirectional Transformer trained on masked language modeling and next sentence prediction.

25Ô∏è‚É£ What is GPT architecture?
Decoder-only Transformer trained with causal language modeling (predict next token).

ü§ñ 5. Generative AI / LLMs

26Ô∏è‚É£ What is fine-tuning?
Training a pre-trained model on domain-specific data for customization.

27Ô∏è‚É£ What is LoRA (Low-Rank Adaptation)?
Trains small adapter matrices instead of full model weights ‚Äî efficient fine-tuning.

28Ô∏è‚É£ What is PEFT (Parameter-Efficient Fine-Tuning)?
Umbrella term for lightweight methods like LoRA, Prefix, and Adapter tuning.

29Ô∏è‚É£ What is RAG (Retrieval-Augmented Generation)?
Combines a retriever (vector DB search) with an LLM to generate fact-based responses.

30Ô∏è‚É£ What are embeddings used for?
Convert text/images into dense vectors for semantic search or similarity.

31Ô∏è‚É£ What is prompt engineering?
Crafting structured prompts to guide model output effectively.

32Ô∏è‚É£ What is temperature in text generation?
Controls randomness ‚Äî low = focused, high = creative.

33Ô∏è‚É£ What are top-k and top-p sampling?
Top-k limits to k highest probabilities; top-p samples until cumulative probability ‚â• p.

34Ô∏è‚É£ What causes hallucinations in LLMs?
When models generate plausible but false information due to lack of grounding or poor context.

35Ô∏è‚É£ How do you reduce hallucinations?
Use RAG, better prompts, fact-checking, and retrieval-based grounding.

‚ö° 6. Model Evaluation & Metrics

36Ô∏è‚É£ What is confusion matrix?
A 2√ó2 matrix showing TP, FP, FN, TN for classification.

37Ô∏è‚É£ What is F1-score?
Harmonic mean of precision and recall.

38Ô∏è‚É£ What are common regression metrics?
MSE, RMSE, MAE, R¬≤.

39Ô∏è‚É£ What is log loss?
Measures how uncertain predictions are compared to actual labels.

40Ô∏è‚É£ How do you handle imbalanced data?
SMOTE, undersampling, class weights, or F1-focused metrics.

üîß 7. MLOps / Deployment

41Ô∏è‚É£ What is MLOps?
End-to-end lifecycle management of ML ‚Äî training ‚Üí deployment ‚Üí monitoring.

42Ô∏è‚É£ How do you deploy an ML model?
Serve via Flask/FastAPI, containerize (Docker), orchestrate (Kubernetes).

43Ô∏è‚É£ What is model drift?
When model performance degrades over time due to data distribution change.

44Ô∏è‚É£ How do you detect drift?
Monitor metrics, input distribution, concept drift tests.

45Ô∏è‚É£ What is model versioning?
Tracking and managing model changes with tools like MLflow or DVC.

üßÆ 8. Practical Engineering & System Design

46Ô∏è‚É£ How would you build a recommendation system?
Collaborative filtering, embeddings, or hybrid model; deploy via API and retrain periodically.

47Ô∏è‚É£ How do you build a chatbot using LLMs?
Use retrieval (FAISS/Pinecone) + LLM (GPT/LLaMA) + prompt templates (LangChain).

48Ô∏è‚É£ How do you optimize model inference speed?
Quantization, batching, caching, distillation, or GPU acceleration.

49Ô∏è‚É£ How do you monitor production ML systems?
Track prediction accuracy, latency, drift, and user feedback.

50Ô∏è‚É£ How do you ensure explainability in ML?
Use SHAP, LIME, attention visualization, and model interpretability tools.